{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac28ed8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Using device: cuda\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT 1: Architecture Comparison\n",
      "============================================================\n",
      "\n",
      "Training SimpleNN_ReLU...\n",
      "SimpleNN_ReLU - Epoch [20/50], Train Loss: 0.493203, Val Loss: 0.422819\n",
      "SimpleNN_ReLU - Epoch [40/50], Train Loss: 0.379580, Val Loss: 0.371813\n",
      "\n",
      "Training SimpleNN_Tanh...\n",
      "SimpleNN_Tanh - Epoch [20/50], Train Loss: 0.466412, Val Loss: 0.478481\n",
      "SimpleNN_Tanh - Epoch [40/50], Train Loss: 0.463965, Val Loss: 0.424147\n",
      "\n",
      "Training SimpleNN_Softplus...\n",
      "SimpleNN_Softplus - Epoch [20/50], Train Loss: 0.618853, Val Loss: 0.551495\n",
      "SimpleNN_Softplus - Epoch [40/50], Train Loss: 0.569432, Val Loss: 0.534677\n",
      "\n",
      "Training RNN...\n",
      "RNN - Epoch [20/50], Train Loss: 0.539400, Val Loss: 0.585711\n",
      "RNN - Epoch [40/50], Train Loss: 0.407453, Val Loss: 0.425974\n",
      "\n",
      "Training GRU...\n",
      "GRU - Epoch [20/50], Train Loss: 0.451486, Val Loss: 0.449456\n",
      "GRU - Epoch [40/50], Train Loss: 0.372406, Val Loss: 0.292346\n",
      "\n",
      "Training LSTM...\n",
      "LSTM - Epoch [20/50], Train Loss: 0.426034, Val Loss: 0.431047\n",
      "LSTM - Epoch [40/50], Train Loss: 0.420597, Val Loss: 0.343636\n",
      "\n",
      "Training PhysicsInformed...\n",
      "PhysicsInformed - Epoch [20/50], Train Loss: 3803097118592205.000000, Val Loss: 3429461055766528.000000\n",
      "PhysicsInformed - Epoch [40/50], Train Loss: 3210644927440486.500000, Val Loss: 2906901780103168.000000\n",
      "\n",
      "Training MaxwellNetwork...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [32]], which is output 0 of AsStridedBackward0, is at version 50; expected version 49 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 662\u001b[39m\n\u001b[32m    656\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m trained_models, results, maxwell_model\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    659\u001b[39m     \u001b[38;5;66;03m# Note: Import the data generator from the previous artifact\u001b[39;00m\n\u001b[32m    660\u001b[39m     \u001b[38;5;66;03m# or load your actual cleaned data\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m     trained_models, results, maxwell_model = \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 631\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    628\u001b[39m runner = RheologyExperimentRunner(df, sequence_length=\u001b[32m50\u001b[39m, batch_size=\u001b[32m32\u001b[39m)\n\u001b[32m    630\u001b[39m \u001b[38;5;66;03m# Run Experiment 1\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m631\u001b[39m trained_models, results, histories = \u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_experiment_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[38;5;66;03m# Visualize results\u001b[39;00m\n\u001b[32m    634\u001b[39m fig = runner.visualize_results(trained_models, results)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 493\u001b[39m, in \u001b[36mRheologyExperimentRunner.run_experiment_1\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    491\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m models.items():\n\u001b[32m    492\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m     trained_model, train_losses, val_losses = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    497\u001b[39m     trained_models[name] = trained_model\n\u001b[32m    498\u001b[39m     training_histories[name] = (train_losses, val_losses)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 358\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, epochs, lr, device, model_name)\u001b[39m\n\u001b[32m    356\u001b[39m outputs = model(batch_x)\n\u001b[32m    357\u001b[39m loss = criterion(outputs, batch_y)\n\u001b[32m--> \u001b[39m\u001b[32m358\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    359\u001b[39m optimizer.step()\n\u001b[32m    361\u001b[39m train_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [32]], which is output 0 of AsStridedBackward0, is at version 50; expected version 49 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# ============================================\n",
    "# PART 1: DATA PREPARATION\n",
    "# ============================================\n",
    "\n",
    "class RheologyDataset(Dataset):\n",
    "    \"\"\"Custom dataset for rheological time series data\"\"\"\n",
    "    \n",
    "    def __init__(self, df, sequence_length=50, stride=10):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: DataFrame with columns [time, strain, strain_rate, stress]\n",
    "            sequence_length: Length of input sequences\n",
    "            stride: Stride for creating overlapping sequences\n",
    "        \"\"\"\n",
    "        self.sequence_length = sequence_length\n",
    "        self.stride = stride\n",
    "        \n",
    "        # Prepare data\n",
    "        self.features = df[['strain', 'strain_rate']].values\n",
    "        self.targets = df['stress'].values\n",
    "        self.time = df['time'].values\n",
    "        \n",
    "        # Normalize\n",
    "        self.scaler_X = StandardScaler()\n",
    "        self.scaler_y = StandardScaler()\n",
    "        \n",
    "        self.features = self.scaler_X.fit_transform(self.features)\n",
    "        self.targets = self.scaler_y.fit_transform(self.targets.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Create sequences\n",
    "        self.sequences = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for i in range(0, len(self.features) - sequence_length, stride):\n",
    "            self.sequences.append(self.features[i:i+sequence_length])\n",
    "            self.labels.append(self.targets[i:i+sequence_length])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (torch.FloatTensor(self.sequences[idx]), \n",
    "                torch.FloatTensor(self.labels[idx]))\n",
    "\n",
    "# ============================================\n",
    "# PART 2: NEURAL NETWORK ARCHITECTURES\n",
    "# ============================================\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    \"\"\"Simple feedforward neural network\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=2, hidden_size=64, output_size=1, \n",
    "                 activation='relu', num_layers=3):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        \n",
    "        # Select activation function\n",
    "        activations = {\n",
    "            'relu': nn.ReLU(),\n",
    "            'tanh': nn.Tanh(),\n",
    "            'sigmoid': nn.Sigmoid(),\n",
    "            'elu': nn.ELU(),\n",
    "            'leaky_relu': nn.LeakyReLU(),\n",
    "            'softplus': nn.Softplus()  # Smooth, good for physical systems\n",
    "        }\n",
    "        self.activation = activations.get(activation, nn.ReLU())\n",
    "        \n",
    "        # Build layers\n",
    "        layers = []\n",
    "        in_features = input_size\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(nn.Linear(in_features, hidden_size))\n",
    "            layers.append(self.activation)\n",
    "            in_features = hidden_size\n",
    "        layers.append(nn.Linear(in_features, output_size))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, features)\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x = x.reshape(batch_size * seq_len, -1)\n",
    "        out = self.network(x)\n",
    "        return out.reshape(batch_size, seq_len, -1).squeeze(-1)\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    \"\"\"Standard RNN model\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=2, hidden_size=64, output_size=1, num_layers=2):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, \n",
    "                         batch_first=True, dropout=0.1 if num_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out)\n",
    "        return out.squeeze(-1)\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    \"\"\"GRU model\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=2, hidden_size=64, output_size=1, num_layers=2):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, \n",
    "                         batch_first=True, dropout=0.1 if num_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru(x)\n",
    "        out = self.fc(out)\n",
    "        return out.squeeze(-1)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"LSTM model\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=2, hidden_size=64, output_size=1, num_layers=2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, \n",
    "                           batch_first=True, dropout=0.1 if num_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out)\n",
    "        return out.squeeze(-1)\n",
    "\n",
    "class PhysicsInformedNN(nn.Module):\n",
    "    \"\"\"Physics-informed neural network with Maxwell constraints\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=2, hidden_size=64, output_size=1):\n",
    "        super(PhysicsInformedNN, self).__init__()\n",
    "        \n",
    "        # Neural network for learning residuals\n",
    "        self.nn = nn.Sequential(\n",
    "            nn.Linear(input_size + 1, hidden_size),  # +1 for previous stress\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        \n",
    "        # Learnable Maxwell parameters\n",
    "        self.log_E = nn.Parameter(torch.tensor([13.0]))  # log(E) for stability\n",
    "        self.log_eta = nn.Parameter(torch.tensor([18.0]))  # log(eta)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        E = torch.exp(self.log_E)\n",
    "        eta = torch.exp(self.log_eta)\n",
    "        tau = eta / E\n",
    "        \n",
    "        stress_pred = torch.zeros(batch_size, seq_len, device=x.device)\n",
    "        stress_prev = torch.zeros(batch_size, device=x.device)\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            strain = x[:, t, 0]\n",
    "            strain_rate = x[:, t, 1]\n",
    "            \n",
    "            # Maxwell model prediction\n",
    "            stress_maxwell = E * strain + eta * strain_rate\n",
    "            \n",
    "            # Neural network correction\n",
    "            nn_input = torch.cat([x[:, t], stress_prev.unsqueeze(1)], dim=1)\n",
    "            correction = self.nn(nn_input).squeeze()\n",
    "            \n",
    "            # Combined prediction\n",
    "            stress_pred[:, t] = stress_maxwell + correction\n",
    "            stress_prev = stress_pred[:, t].detach()\n",
    "        \n",
    "        return stress_pred\n",
    "\n",
    "# ============================================\n",
    "# PART 3: CUSTOM MAXWELL NEURON\n",
    "# ============================================\n",
    "\n",
    "class MaxwellNeuron(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Maxwell element as a neural network component\n",
    "    This can be stacked to create generalized Maxwell models\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, init_E=1e6, init_eta=1e8, learnable=True):\n",
    "        super(MaxwellNeuron, self).__init__()\n",
    "        \n",
    "        if learnable:\n",
    "            self.log_E = nn.Parameter(torch.log(torch.tensor([init_E])))\n",
    "            self.log_eta = nn.Parameter(torch.log(torch.tensor([init_eta])))\n",
    "        else:\n",
    "            self.log_E = torch.log(torch.tensor([init_E]))\n",
    "            self.log_eta = torch.log(torch.tensor([init_eta]))\n",
    "    \n",
    "    def forward(self, strain, strain_rate, dt=0.1):\n",
    "        \"\"\"\n",
    "        Forward pass through Maxwell element\n",
    "        Args:\n",
    "            strain: (batch, seq_len)\n",
    "            strain_rate: (batch, seq_len)\n",
    "            dt: time step\n",
    "        \"\"\"\n",
    "        E = torch.exp(self.log_E)\n",
    "        eta = torch.exp(self.log_eta)\n",
    "        tau = eta / E\n",
    "        \n",
    "        batch_size, seq_len = strain.shape\n",
    "        stress = torch.zeros_like(strain)\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            if t == 0:\n",
    "                stress[:, t] = E * strain[:, t] + eta * strain_rate[:, t]\n",
    "            else:\n",
    "                # Maxwell differential equation discretized\n",
    "                stress[:, t] = stress[:, t-1] * torch.exp(-dt/tau) + \\\n",
    "                              E * (strain[:, t] - strain[:, t-1])\n",
    "        \n",
    "        return stress\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        \"\"\"Return physical parameters\"\"\"\n",
    "        E = torch.exp(self.log_E).item()\n",
    "        eta = torch.exp(self.log_eta).item()\n",
    "        return {'E': E, 'eta': eta, 'tau': eta/E}\n",
    "\n",
    "class GeneralizedMaxwellNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Network of Maxwell elements in parallel\n",
    "    This represents a generalized Maxwell model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_elements=10, input_size=2):\n",
    "        super(GeneralizedMaxwellNetwork, self).__init__()\n",
    "        \n",
    "        # Create multiple Maxwell elements\n",
    "        self.maxwell_elements = nn.ModuleList([\n",
    "            MaxwellNeuron(\n",
    "                init_E=10**(np.random.uniform(5, 7)),\n",
    "                init_eta=10**(np.random.uniform(7, 9))\n",
    "            ) for _ in range(n_elements)\n",
    "        ])\n",
    "        \n",
    "        # Learnable weights for combining elements\n",
    "        self.element_weights = nn.Parameter(torch.ones(n_elements) / n_elements)\n",
    "        \n",
    "        # Optional: Gating mechanism to sparsify the network\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(input_size, n_elements),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, dt=0.1):\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len, 2) - strain and strain_rate\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        strain = x[:, :, 0]\n",
    "        strain_rate = x[:, :, 1]\n",
    "        \n",
    "        # Compute gating values\n",
    "        gate_values = self.gate(x.mean(dim=1))  # (batch, n_elements)\n",
    "        \n",
    "        # Compute stress from each Maxwell element\n",
    "        stresses = []\n",
    "        for i, element in enumerate(self.maxwell_elements):\n",
    "            stress = element(strain, strain_rate, dt)\n",
    "            # Apply gating and weighting\n",
    "            weighted_stress = stress * self.element_weights[i] * gate_values[:, i].unsqueeze(1)\n",
    "            stresses.append(weighted_stress)\n",
    "        \n",
    "        # Combine all stresses\n",
    "        total_stress = torch.stack(stresses).sum(dim=0)\n",
    "        \n",
    "        return total_stress\n",
    "    \n",
    "    def get_element_contributions(self, x, dt=0.1):\n",
    "        \"\"\"Get contribution of each Maxwell element\"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        strain = x[:, :, 0]\n",
    "        strain_rate = x[:, :, 1]\n",
    "        \n",
    "        gate_values = self.gate(x.mean(dim=1))\n",
    "        \n",
    "        contributions = []\n",
    "        parameters = []\n",
    "        \n",
    "        for i, element in enumerate(self.maxwell_elements):\n",
    "            stress = element(strain, strain_rate, dt)\n",
    "            weight = self.element_weights[i] * gate_values[:, i].mean()\n",
    "            \n",
    "            contributions.append({\n",
    "                'element_id': i,\n",
    "                'weight': weight.item(),\n",
    "                'avg_stress': stress.mean().item(),\n",
    "                'contribution': (stress * weight).mean().item()\n",
    "            })\n",
    "            \n",
    "            params = element.get_parameters()\n",
    "            params['element_id'] = i\n",
    "            params['weight'] = weight.item()\n",
    "            parameters.append(params)\n",
    "        \n",
    "        return contributions, parameters\n",
    "    \n",
    "    def prune_network(self, threshold=0.01):\n",
    "        \"\"\"Remove elements with low contribution\"\"\"\n",
    "        with torch.no_grad():\n",
    "            weights = torch.abs(self.element_weights)\n",
    "            mask = weights > threshold\n",
    "            self.element_weights.data *= mask.float()\n",
    "        \n",
    "        active_elements = mask.sum().item()\n",
    "        print(f\"Active elements after pruning: {active_elements}/{len(self.maxwell_elements)}\")\n",
    "        \n",
    "        return mask\n",
    "\n",
    "# ============================================\n",
    "# PART 4: TRAINING AND EVALUATION\n",
    "# ============================================\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=100, lr=0.001, \n",
    "                device='cpu', model_name='Model'):\n",
    "    \"\"\"Train a model and return training history\"\"\"\n",
    "    \n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f'{model_name} - Epoch [{epoch+1}/{epochs}], '\n",
    "                  f'Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')\n",
    "    \n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "def evaluate_models(models_dict, test_loader, device='cpu'):\n",
    "    \"\"\"Evaluate all models on test set\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    for name, model in models_dict.items():\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        predictions = []\n",
    "        targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in test_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                test_loss += loss.item()\n",
    "                \n",
    "                predictions.append(outputs.cpu().numpy())\n",
    "                targets.append(batch_y.cpu().numpy())\n",
    "        \n",
    "        test_loss /= len(test_loader)\n",
    "        \n",
    "        predictions = np.concatenate(predictions)\n",
    "        targets = np.concatenate(targets)\n",
    "        \n",
    "        # Calculate additional metrics\n",
    "        mse = np.mean((predictions - targets) ** 2)\n",
    "        mae = np.mean(np.abs(predictions - targets))\n",
    "        r2 = 1 - (np.sum((targets - predictions) ** 2) / \n",
    "                  np.sum((targets - targets.mean()) ** 2))\n",
    "        \n",
    "        results[name] = {\n",
    "            'test_loss': test_loss,\n",
    "            'mse': mse,\n",
    "            'mae': mae,\n",
    "            'r2': r2,\n",
    "            'predictions': predictions,\n",
    "            'targets': targets\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ============================================\n",
    "# PART 5: EXPERIMENT RUNNER\n",
    "# ============================================\n",
    "\n",
    "class RheologyExperimentRunner:\n",
    "    \"\"\"Main class to run experiments\"\"\"\n",
    "    \n",
    "    def __init__(self, df, sequence_length=50, batch_size=32):\n",
    "        self.df = df\n",
    "        self.sequence_length = sequence_length\n",
    "        self.batch_size = batch_size\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Prepare datasets\n",
    "        self.prepare_data()\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        \"\"\"Prepare train, validation, and test datasets\"\"\"\n",
    "        \n",
    "        # Create dataset\n",
    "        dataset = RheologyDataset(self.df, self.sequence_length)\n",
    "        \n",
    "        # Split data\n",
    "        n_samples = len(dataset)\n",
    "        train_size = int(0.7 * n_samples)\n",
    "        val_size = int(0.15 * n_samples)\n",
    "        test_size = n_samples - train_size - val_size\n",
    "        \n",
    "        train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "            dataset, [train_size, val_size, test_size]\n",
    "        )\n",
    "        \n",
    "        # Create data loaders\n",
    "        self.train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        self.val_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "        self.test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "    \n",
    "    def run_experiment_1(self):\n",
    "        \"\"\"\n",
    "        Experiment 1: Compare different neural network architectures\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"EXPERIMENT 1: Architecture Comparison\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        models = {\n",
    "            'SimpleNN_ReLU': SimpleNN(activation='relu'),\n",
    "            'SimpleNN_Tanh': SimpleNN(activation='tanh'),\n",
    "            'SimpleNN_Softplus': SimpleNN(activation='softplus'),\n",
    "            'RNN': RNNModel(),\n",
    "            'GRU': GRUModel(),\n",
    "            'LSTM': LSTMModel(),\n",
    "            'PhysicsInformed': PhysicsInformedNN(),\n",
    "            'MaxwellNetwork': GeneralizedMaxwellNetwork(n_elements=5)\n",
    "        }\n",
    "        \n",
    "        trained_models = {}\n",
    "        training_histories = {}\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            print(f\"\\nTraining {name}...\")\n",
    "            trained_model, train_losses, val_losses = train_model(\n",
    "                model, self.train_loader, self.val_loader,\n",
    "                epochs=50, lr=0.001, device=self.device, model_name=name\n",
    "            )\n",
    "            trained_models[name] = trained_model\n",
    "            training_histories[name] = (train_losses, val_losses)\n",
    "        \n",
    "        # Evaluate all models\n",
    "        print(\"\\n\" + \"=\"*40)\n",
    "        print(\"Model Evaluation Results:\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        results = evaluate_models(trained_models, self.test_loader, self.device)\n",
    "        \n",
    "        # Print comparison table\n",
    "        print(f\"\\n{'Model':<20} {'MSE':<12} {'MAE':<12} {'R²':<12}\")\n",
    "        print(\"-\" * 56)\n",
    "        \n",
    "        for name, metrics in results.items():\n",
    "            print(f\"{name:<20} {metrics['mse']:<12.6f} {metrics['mae']:<12.6f} {metrics['r2']:<12.6f}\")\n",
    "        \n",
    "        # Find best model\n",
    "        best_model = min(results.items(), key=lambda x: x[1]['mse'])[0]\n",
    "        print(f\"\\nBest performing model: {best_model}\")\n",
    "        \n",
    "        return trained_models, results, training_histories\n",
    "    \n",
    "    def run_experiment_2(self, n_elements=10):\n",
    "        \"\"\"\n",
    "        Experiment 2: Train Generalized Maxwell Network and analyze components\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"EXPERIMENT 2: Generalized Maxwell Network Analysis\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Create and train the network\n",
    "        model = GeneralizedMaxwellNetwork(n_elements=n_elements)\n",
    "        \n",
    "        print(f\"\\nTraining Generalized Maxwell Network with {n_elements} elements...\")\n",
    "        trained_model, train_losses, val_losses = train_model(\n",
    "            model, self.train_loader, self.val_loader,\n",
    "            epochs=100, lr=0.001, device=self.device,\n",
    "            model_name='GeneralizedMaxwell'\n",
    "        )\n",
    "        \n",
    "        # Analyze element contributions\n",
    "        print(\"\\n\" + \"=\"*40)\n",
    "        print(\"Element Analysis:\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        # Get a sample batch for analysis\n",
    "        sample_batch = next(iter(self.test_loader))\n",
    "        sample_x, sample_y = sample_batch\n",
    "        sample_x = sample_x.to(self.device)\n",
    "        \n",
    "        contributions, parameters = trained_model.get_element_contributions(sample_x)\n",
    "        \n",
    "        # Sort by contribution\n",
    "        contributions.sort(key=lambda x: abs(x['contribution']), reverse=True)\n",
    "        \n",
    "        print(f\"\\n{'Element':<10} {'Weight':<12} {'E (Pa)':<12} {'η (Pa·s)':<12} {'τ (s)':<12} {'Contribution':<12}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        for i, (contrib, params) in enumerate(zip(contributions[:10], parameters[:10])):\n",
    "            if contrib['weight'] > 0.01:  # Only show significant elements\n",
    "                print(f\"{params['element_id']:<10} {params['weight']:<12.4f} \"\n",
    "                      f\"{params['E']:<12.2e} {params['eta']:<12.2e} \"\n",
    "                      f\"{params['tau']:<12.4f} {contrib['contribution']:<12.6f}\")\n",
    "        \n",
    "        # Prune network\n",
    "        print(\"\\n\" + \"=\"*40)\n",
    "        print(\"Network Pruning:\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        mask = trained_model.prune_network(threshold=0.05)\n",
    "        \n",
    "        # Re-evaluate after pruning\n",
    "        trained_model.eval()\n",
    "        test_loss = 0\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in self.test_loader:\n",
    "                batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)\n",
    "                outputs = trained_model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                test_loss += loss.item()\n",
    "        \n",
    "        test_loss /= len(self.test_loader)\n",
    "        print(f\"Test loss after pruning: {test_loss:.6f}\")\n",
    "        \n",
    "        return trained_model, contributions, parameters\n",
    "    \n",
    "    def visualize_results(self, models, results):\n",
    "        \"\"\"Visualize model predictions vs actual\"\"\"\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for idx, (name, metrics) in enumerate(list(results.items())[:8]):\n",
    "            ax = axes[idx]\n",
    "            \n",
    "            # Plot first 200 predictions\n",
    "            pred_sample = metrics['predictions'].flatten()[:200]\n",
    "            target_sample = metrics['targets'].flatten()[:200]\n",
    "            \n",
    "            ax.plot(target_sample, label='Actual', alpha=0.7)\n",
    "            ax.plot(pred_sample, label='Predicted', alpha=0.7)\n",
    "            ax.set_title(f'{name}\\nR² = {metrics[\"r2\"]:.4f}')\n",
    "            ax.set_xlabel('Time Step')\n",
    "            ax.set_ylabel('Stress')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return fig\n",
    "\n",
    "# ============================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================\n",
    "\n",
    "def main():\n",
    "    # Load your data\n",
    "    print(\"Loading data...\")\n",
    "    # Assuming you have your cleaned ramp experiment data\n",
    "    # df = pd.read_csv('your_cleaned_ramp_data.csv')\n",
    "    \n",
    "    # For demonstration, let's create sample data\n",
    "    # You should replace this with your actual data loading\n",
    "    \n",
    "    df = pd.read_csv('dataset.csv')\n",
    "    \n",
    "    # Initialize experiment runner\n",
    "    runner = RheologyExperimentRunner(df, sequence_length=50, batch_size=32)\n",
    "    \n",
    "    # Run Experiment 1\n",
    "    trained_models, results, histories = runner.run_experiment_1()\n",
    "    \n",
    "    # Visualize results\n",
    "    fig = runner.visualize_results(trained_models, results)\n",
    "    \n",
    "    # Run Experiment 2\n",
    "    maxwell_model, contributions, parameters = runner.run_experiment_2(n_elements=10)\n",
    "    \n",
    "    # Plot training histories\n",
    "    fig2, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, (name, (train_losses, val_losses)) in enumerate(list(histories.items())[:8]):\n",
    "        ax = axes[idx]\n",
    "        ax.plot(train_losses, label='Train', alpha=0.7)\n",
    "        ax.plot(val_losses, label='Validation', alpha=0.7)\n",
    "        ax.set_title(name)\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return trained_models, results, maxwell_model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Note: Import the data generator from the previous artifact\n",
    "    # or load your actual cleaned data\n",
    "    \n",
    "    trained_models, results, maxwell_model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13252c2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
